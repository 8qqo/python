/*
The following is the annotation area.
類神經網路（Artificial Neural Networks, ANN）是模仿生物神經系統結構和功能的數學模型。它們廣泛應用於各種人工智能任務中，如圖像識別、語音識別、自然語言處理和自動駕駛等。以下是類神經網路的基礎概念及其在 AI 中的應用：

1. 類神經網路的結構

	•	神經元（Neuron）：神經網路的基本單元，類似於生物神經元。每個神經元接收來自其他神經元的輸入，經過加權和激活函數的處理後，產生輸出。
	•	輸入層（Input Layer）：接收外部數據的層，將數據傳遞給隱藏層。
	•	隱藏層（Hidden Layers）：介於輸入層和輸出層之間的層，通過多層的線性變換和非線性激活函數，模型可以學習複雜的模式。
	•	輸出層（Output Layer）：最終輸出的層，根據任務類型輸出結果（如分類、回歸）。
	•	權重（Weights）和偏差（Biases）：神經元間的連接由權重決定，每個神經元也可以有偏差。訓練過程就是不斷更新權重和偏差的過程。

2. 激活函數（Activation Function）

	•	激活函數決定了神經元的輸出。常見的激活函數有：
	•	ReLU（Rectified Linear Unit）：最常用的激活函數，將輸入中的負值置為 0，正值保持不變。
	•	Sigmoid：輸出範圍為 0 到 1，常用於二元分類問題。
	•	Tanh：輸出範圍為 -1 到 1，通常在某些情況下比 Sigmoid 更有效。

3. 訓練過程

	•	前向傳播（Forward Propagation）：數據從輸入層經過隱藏層傳遞到輸出層，並通過激活函數進行非線性變換，最終得到輸出結果。
	•	損失函數（Loss Function）：衡量預測結果與真實結果的差異。常見的損失函數包括：
	•	均方誤差（MSE）：用於回歸問題，衡量預測值與實際值的平方差。
	•	交叉熵（Cross-Entropy）：用於分類問題，衡量預測分佈與真實分佈之間的差異。
	•	反向傳播（Backpropagation）：根據損失函數的值，計算梯度，並通過優化器（如梯度下降法）來更新模型的權重和偏差。
	•	優化器（Optimizer）：常用的優化器包括 SGD（隨機梯度下降）、Adam 等，它們負責以合適的步伐更新權重，使損失函數最小化。

4. 深度神經網路（DNN）

當神經網路有多個隱藏層時，它被稱為深度神經網路。深度學習是目前類神經網路中最熱門的研究領域，能夠學習更複雜的數據結構，處理大量數據並獲得極高的精度。

5. 應用場景

	•	圖像識別：CNN（卷積神經網路）是一種特殊的神經網路，用於圖像數據的特徵提取和分類，廣泛應用於人臉識別、醫學影像處理等領域。
	•	語音識別：RNN（循環神經網路）適用於處理序列數據，如語音或文本。LSTM 和 GRU 是 RNN 的改進版本，解決了長期依賴問題，應用於語音識別和語言模型中。
	•	自然語言處理（NLP）：BERT 和 GPT 等預訓練模型在文本分類、機器翻譯和文本生成等任務中表現優異。

 */

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# 1. 定義神經網絡模型
class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 128)  # 第一層，全連接層
        self.fc2 = nn.Linear(128, 10)  # 第二層，輸出10類
        
    def forward(self, x):
        x = x.view(-1, 28 * 28)  # 改變輸入形狀為一維向量
        x = torch.relu(self.fc1(x))  # 使用 ReLU 激活函數
        x = self.fc2(x)  # 輸出不加激活
        return x

# 2. 加載 MNIST 數據集
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])

train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)

train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)

# 3. 初始化模型、損失函數和優化器
model = SimpleNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 4. 訓練模型
def train(model, train_loader, criterion, optimizer, num_epochs=5):
    for epoch in range(num_epochs):
        for images, labels in train_loader:
            # 前向傳播
            outputs = model(images)
            loss = criterion(outputs, labels)

            # 反向傳播和優化
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# 5. 開始訓練
train(model, train_loader, criterion, optimizer)

# 6. 測試模型準確度
def test(model, test_loader):
    model.eval()  # 設置模型為評估模式
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in test_loader:
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print(f'Accuracy: {100 * correct / total}%')

test(model, test_loader)